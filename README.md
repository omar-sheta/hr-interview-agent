# HR Interview Agent - Client-Server Architecture

ğŸ¯ **AI-Powered Interview Assistant with GPU Acceleration**

A comprehensive, privacy-focused HR interview system featuring real-time speech recognition, AI-powered question generation, intelligent scoring using Gemma model, and **high-quality natural text-to-speech** capabilities.

![HR Interview Agent](assets/HIVE-logo-4-color.png)

## ğŸŒŸ Features

- **ğŸ™ï¸ Real-time Speech Recognition** - GPU-accelerated MLX-Whisper for instant transcription
- **ğŸ§  AI-Powered Scoring** - Gemma 3:27B model evaluates responses using structured HR rubrics
- **ğŸ”Š High-Quality Text-to-Speech** - Piper high-quality voice synthesis (Lessac, Ryan, Joe, Bryce voices)
- **ğŸ“± Modern Web Interface** - Responsive React frontend with real-time feedback
- **ğŸ”’ Privacy-First** - All processing runs locally, no data leaves your machine
- **âš¡ GPU Acceleration** - Optimized for Apple Silicon and CUDA GPUs
- **ğŸ“Š Comprehensive Analytics** - Detailed scoring with linguistic and behavioral competency analysis
- **ğŸ—ï¸ Client-Server Architecture** - Scalable FastAPI backend with persistent data storage

## ğŸ—ï¸ Architecture

The client-server architecture separates concerns for better scalability and maintainability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       HTTP API        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Client      â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚     Server      â”‚
â”‚  (Web/Python)   â”‚                       â”‚   (FastAPI)     â”‚
â”‚                 â”‚                       â”‚                 â”‚
â”‚  - UI/UX        â”‚                       â”‚  - STT (MLX)    â”‚
â”‚  - Audio        â”‚                       â”‚  - TTS (Piper)  â”‚
â”‚  - Recording    â”‚                       â”‚  - LLM (Gemma)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits

- **ğŸ”’ Privacy-First**: All AI processing runs locally, no data leaves your network
- **âš¡ Performance**: Optimized with Apple Silicon MLX acceleration
- **ğŸŒ Network-Ready**: HTTPS support for cross-device microphone access
- **ğŸ¯ User-Friendly**: Intelligent auto-stop recording and seamless audio experience
- **ğŸ”§ Flexible**: Multiple client types (web, Python) with RESTful API
- **ğŸ“¦ Self-Contained**: Minimal dependencies, easy deployment
- **ğŸ†“ Open Source**: Built entirely with open-source models and frameworks

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.12+**
- **Node.js 18+** (for development)
- **Ollama** with Gemma 3:27B model
- **Apple Silicon Mac** (recommended) or CUDA-capable GPU

### ğŸš€ Option 1: Quick Start (Recommended)

```bash
cd client_server
./quick_start.sh
```

**This will:**
- Start the FastAPI server on port 8001
- Host the web client at `http://localhost:8080/client_server/client/index.html`
- Generate a self-signed certificate (if needed) and also serve `https://localhost:8443/client_server/client/index.html` so browsers allow microphone access on other devices
- Automatically open the hosted web client in your browser
- Display both local and LAN URLs for easy access

> âš ï¸ The HTTPS endpoint uses a self-signed certificate. On Safari/iOS, click "Show Details" â†’ "visit this website" the first time to trust it; Chrome will prompt similarly. Once trusted, microphone access works across your network.

### âš™ï¸ Option 2: Advanced Script with Options

```bash
cd client_server
./start_client_server.sh [start|stop|clean|help]
```

The script hosts the API on `0.0.0.0:8001` and the web UI on `0.0.0.0:8080`, so teammates on the same network can reach it using your machine's IP address.

### ğŸ”§ Option 3: Manual Start

```bash
# Start server
cd client_server/server
python3 main.py

# Serve the web client (terminal 2)
cd ../..
python3 -m http.server 8080 --bind 0.0.0.0

# Open the hosted UI
open http://localhost:8080/client_server/client/index.html
```

## ğŸ¯ Interview Experience Upgrades

### ğŸ¤ Smart Auto-Stop Recording
- **Intelligent Timer**: Auto-stop timer only starts after user begins speaking (not immediately)
- **Audio Processing**: Auto-stopped recordings are processed instead of being discarded
- **Positive Feedback**: Shows "Processing your response..." instead of error messages
- **5-Second Detection**: Consistent 5-second duration for both silence and noise detection
- **No Wasted Audio**: System captures and uses whatever audio was recorded

### ğŸµ Enhanced Audio Experience
- Question audio playback halts when recording or skipping
- Each question autoplays on arrival for seamless flow
- Candidates can redo questions before committing their response
- Upload audio files as an alternative to live recording

### ğŸŒ Network & Connectivity
- Automatic API host detection for network testing
- URL parameter support (`?api_host=SERVER_IP`) for cross-device access
- Clean, distraction-free interface without unnecessary troubleshooting panels
- HTTPS support with self-signed certificates for microphone access across networks

## ğŸŒ Access URLs

After starting with any method above:

- **ğŸ¯ Web Client (local)**: `http://localhost:8080/client_server/client/index.html`
- **ğŸ¯ Web Client (LAN)**: `http://<your-ip>:8080/client_server/client/index.html`
- **ğŸ”§ API Server (local)**: `http://localhost:8001`
- **ğŸ”§ API Server (LAN)**: `http://<your-ip>:8001`
- **ğŸ“š API Documentation**: `http://localhost:8001/docs`
- **â¤ï¸ Health Check**: `http://localhost:8001/health`

> Tip: Run `./start_client_server.sh` to have all URLs printed (including your LAN IP) automatically.

### Use Python Client

```bash
cd client_server/client
python3 hr_client.py
```

## ğŸš€ Performance & Compatibility

### Recommended System Specifications
- **RAM**: 8GB minimum, 16GB recommended (with local LLM)
- **Storage**: 5GB free space
- **CPU**: Multi-core processor (Apple Silicon preferred for MLX)
- **Network**: Stable connection for initial model downloads

### Platform Support
- **macOS**: Full MLX acceleration (Apple Silicon)
- **Linux**: CPU fallback with good performance
- **Windows**: CPU fallback supported
- **Web Browsers**: Chrome, Safari, Firefox, Edge (microphone required)

### Model Performance (Apple Silicon)
- **Transcription**: <2 seconds for 30-second audio
- **TTS Generation**: <1 second for typical questions
- **Memory Footprint**: Optimized for efficiency
- **Startup Time**: ~10 seconds (models preloaded)

## ğŸ“ Project Structure

```
hr_agent_final_attempt/
â”œâ”€â”€ hr_agent/                  # Core FastAPI application
â”‚   â”œâ”€â”€ api/                   # API route handlers
â”‚   â”‚   â”œâ”€â”€ interviews.py      # Interview session management
â”‚   â”‚   â”œâ”€â”€ questions.py       # Question generation
â”‚   â”‚   â”œâ”€â”€ scoring.py         # AI-powered scoring system
â”‚   â”‚   â”œâ”€â”€ tts.py            # High-quality TTS (Piper voices)
â”‚   â”‚   â””â”€â”€ stt_mlx.py        # Speech-to-text (MLX-Whisper)
â”‚   â”œâ”€â”€ data/                  # Session storage
â”‚   â”‚   â””â”€â”€ sessions/          # Interview session files
â”‚   â”œâ”€â”€ uploads/               # Audio response files
â”‚   â”œâ”€â”€ config.py             # Configuration settings
â”‚   â”œâ”€â”€ models.py             # ML model management
â”‚   â””â”€â”€ main.py               # FastAPI application entry point
â”œâ”€â”€ client_server/            # Production client-server setup
â”‚   â”œâ”€â”€ server/                # FastAPI server with data persistence
â”‚   â”‚   â”œâ”€â”€ main.py           # Server application
â”‚   â”‚   â”œâ”€â”€ data_manager.py   # Persistent data management
â”‚   â”‚   â”œâ”€â”€ piper_voices/     # High-quality TTS voice models
â”‚   â”‚   â””â”€â”€ data/             # Persistent interview data
â”‚   â”œâ”€â”€ client/                # Web client application
â”‚   â”‚   â”œâ”€â”€ index.html        # Interview interface
â”‚   â”‚   â””â”€â”€ hr_client.py      # Python client (optional)
â”‚   â”œâ”€â”€ start_client_server.sh # Production launcher
â”‚   â””â”€â”€ README.md             # Client-server documentation
â”œâ”€â”€ frontend/                  # Development React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx           # Main application component
â”‚   â”‚   â”œâ”€â”€ main.jsx          # React entry point
â”‚   â”‚   â””â”€â”€ index.css         # Tailwind CSS styles
â”‚   â”œâ”€â”€ public/               # Static assets
â”‚   â””â”€â”€ package.json          # Node.js dependencies
â”œâ”€â”€ piper_voices/             # TTS voice models (Lessac, Ryan, Joe, Bryce)
â”œâ”€â”€ assets/                   # Static assets and logos
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ start_dev.sh             # Development server launcher
â””â”€â”€ README.md                # This documentation
```

## ğŸ¤– Open-Source Models & Technologies

### Speech-to-Text (STT)
**MLX-Whisper**
- **Model**: OpenAI Whisper (various sizes: tiny, base, small, medium, large)
- **Memory**: 39MB (tiny) to 3.09GB (large-v3)
- **License**: MIT License
- **Optimization**: Apple Silicon (MLX) acceleration
- **Fallback**: OpenAI Whisper (CPU) for non-Apple Silicon systems
- **Repository**: [ml-explore/mlx-whisper](https://github.com/ml-explore/mlx-whisper)

### Text-to-Speech (TTS)
**Piper TTS**
- **Model**: Lessac (High Quality)
- **File Size**: ~63MB (en_US-lessac-high.onnx)
- **Memory Usage**: ~100-200MB during synthesis
- **License**: MIT License
- **Format**: ONNX optimized neural voice
- **Repository**: [rhasspy/piper](https://github.com/rhasspy/piper)
- **Voice Quality**: Professional, natural, human-like speech synthesis

### Language Model (LLM)
**Gemma 3:27B**
- **Model**: Google Gemma 3 (27B parameters)
- **Memory**: ~16GB VRAM required
- **License**: Apache 2.0 License
- **Integration**: Ollama for local inference
- **Repository**: [google/gemma](https://github.com/google/gemma)

### System Requirements by Model

| Component | Memory Usage | Disk Space | License |
|-----------|-------------|------------|---------|
| MLX-Whisper (base) | ~1GB | ~150MB | MIT |
| Piper TTS (Lessac High) | ~200MB | ~63MB | MIT |
| Gemma 27B (optional) | ~16GB | ~16GB | Apache 2.0 |
| **Total Minimum** | **~1.2GB** | **~213MB** | **Mixed** |
| **With LLM** | **~17.2GB** | **~16.2GB** | **Mixed** |

## API Endpoints

The server exposes RESTful endpoints:

- `POST /transcribe` - Speech to text (MLX-Whisper)
- `POST /synthesize` - Text to speech (Piper TTS)
- `POST /generate` - LLM text generation (Ollama integration)
- `POST /interview/start` - Start interview session
- `POST /interview/submit` - Submit response
- `GET /health` - Server health check
- `GET /models/status` - Model loading status

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **MLX** team for Apple Silicon optimization
- **Piper TTS** for high-quality voice synthesis
- **Ollama** for local LLM inference
- **Meta** for the Gemma language model
- **FastAPI** and **React** communities

---